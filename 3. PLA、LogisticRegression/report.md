# 人工智能实验三 实验报告

18340057  18级计算机科学二班

## 一、 感知机PLA

### 1. 算法原理

感知机针对二分类问题进行学习。输入为样本的特征向量$\bold x\in R^n$，输出为类别$y\in\{+1,-1\}$。

感知机的预测结果为$f(x)=sign(w\cdot x+b)$，其中$sign$为符号函数，在自变量大于0时为1，自变量小于0时为-1。$w$为权值向量，和x的维度相同。b是偏置，为标量。线性方程$w\cdot x+b$对应着特征空间的一个分离超平面，将特征空间分成两个部分，位于两部分的点分别对应正样本和负样本。

感知机利用损失函数对模型参量w和b进行更新学习。将误分类点$(x_i,y_i)$到分离超平面的距离定义为：
$$
\frac{1}{||w||}||w\cdot x+b||=-\frac{1}{||w||}(w\cdot x_i+b)y_i
$$
假设误分类点的集合为M，且不考虑$-\frac{1}{||w||}$则损失函数为：
$$
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$
可以采用随机梯度下降的方式来优化损失函数。w和b的损失函数梯度分别为：
$$
\nabla_wL(w,b)=-\sum_{x_i\in M}y_ix_i
$$

$$
\nabla_bL(w,b)=-\sum_{x_i\in M}y_i
$$

选择某个误分类点对参数进行更新即可：
$$
w=w+\eta y_ix_i
$$

$$
b=b+\eta y_i
$$

其中$\eta$表示学习率，由自己定值。

-----

### 2. 流程图和伪代码

![](pic\\1.png)

从`csv`文件读取样本的过程不再赘述。训练集存在列表`trainSet`中，每个元素为一个样本，每个样本为一个向量，有41维，前40维为40个不同特征的特征值，第41维为标签0或1。为了方便训练过程，我将标签0在读入数据时改为了-1。

设置学习率和初始化`w`和`b`：

```pseudocode
    w = [ 0 for i in range(40)]
    b = 0
    learningRate = 1
```

在训练过程中，对每个分类点进行检索并找出误分类点：

```pseudocode
/* 
input: 数据集trainSet 学习率learningRate 模型参量w和b
output: 训练好的w和b 
*/
def train(trainSet, learningRate, w, b){
    times = 0
    while (times <= 最大迭代次数)
        times += 1
        w, b = PLA(trainSet, learningRate, w, b)
    end
    return w, b
}
```

需要注意的是，当训练集线性不可分时，PLA算法不可能将所有的分类点准确地分成两个部分，每次必然会出现误分类点。如果训练到不出现误分类点时才停止，程序将不会停止。所以需要设置最大迭代次数，即上面代码设定的`times`。每次迭代时，`times`加一，当递归次数达到最大值时停止训练。

每次迭代过程的操作如下：

```pseudocode
/*
input: 数据集trainSet 学习率learningRate 模型参量w和b
output: 每次迭代后的的w和b 
*/
def PLA(trainSet, learningRate, w, b){
    for eachSample in trainSet:
        num = w和eachSample的向量乘积 + b
        /* 出现误分类点，更新参量后直接返回 */
        if 当前点为误分类点 then
            w = w + learningRate*x*y
            b = b + learningRate*y
            return w, b
    end
    /* 没有出现误分类点，返回训练好的w和b */
    return w, b
}
```

验证和测试过程只需要用向量w乘以特征向量x后加上b，依据结果的正负判断预测值即可，这里不再赘述。

----

### 3. 代码展示

首先是对训练集的读取：

```python
def readTrainSet():
    trainSet = []
    with open('train.csv','r') as ts:
        for eachLine in ts:
            s = eachLine.split(',')
            # 去掉最后的回车
            s[40] = s[40][0:-1]
            # 进行数据类型转换，从字符串转换成数字
            for i in range(40):
                s[i] = float(s[i])
            s[40] = int(s[40])
            # 将0的标签改为-1，方便训练过程
            if s[40] == 0:
                s[40] = -1
            # 加入记录训练集的列表
            trainSet.append(s)
    return trainSet
```

这里特别列出训练集的读取，因为有以下操作需要注意：

- 将训练样本的特征向量值转换为`float`方便之后预测结果的计算过程
- 将训练样本的标签的`0`改为`-1`，`1`保持不变，方便之后对权值的更新

- 特征值和标签存储在一个列表中。`trainSet`的每一项对应一个样本的列表，前40项为40个特征值，第41个为标签

训练时调用以下函数：

```python
def train(trainSet, learningRate):
    w = [ 0 for i in range(40)]
    b = 0
    times = 0
    while times <= 10000:
        times += 1
        w, b = PLA(trainSet, learningRate, w, b)
    return w, b
```

初始化权重向量w和偏置值b为0，在限制的迭代次数内进行w和b的更新即可。

每次迭代的具体过程如下：

```python
def PLA(trainSet, learningRate, w, b):
    # 遍历每个样本
    for eachSample in trainSet:
        # num用来存储对当前样本的预测值
        num = 0
        for i in range(40):
            num = num + w[i] * eachSample[i]
        num += b
        # 预测结果不对的三种情况：预测值为0、预测值为1但实际为-1、预测值为-1但实际为1
        if num == 0 or (num > 0 and eachSample[40] == -1) or (num < 0 and eachSample[40] == 1):
            # 若为误分类点则更新参数并返回
            for j in range(40):
                w[j] = w[j] + eachSample[40] * eachSample[j] * learningRate
            b += (learningRate * eachSample[40])
            return w, b
    return w, b
```

最后只需要调用以下函数即可完成训练和验证过程：

```python
trainSet = readTrainSet()
learningRate = 1
w, b = train(trainSet, learningRate)
valid(w, b)
```

------

### 4. 创新点

在上述的PLA算法中，我尝试在每次出现误分类点时，输出当前误分类点的编号。我发现，在几千条数据样本中，总是在第几十条或是几百条的时候碰到误分类点。每次更新后重新迭代，又要从首个样本开始判断，这就导致了一个问题：在整个**数千条**的训练样本中，只有**前几百条**被反复训练了，直到达到最大迭代次数，之后的几千条样本完全都不会训练到。对此，我进行了改进。

考虑到数据集是线性不可分的，有可能是某些值比较极端的点导致了误分类，也就是每次都在某些点上误分类。如果能将这些点排除掉，也许能够改进整个模型的精确度。对此，我进行了以下改进：

- 每次碰到误分类点时，记录误分类点的编号，以及该点的误分类次数
- 设置误分类次数的最大值
- 当某个误分类点的误分类次数达到最大值时，将这个点从训练样本集中剔除

最大误分类次数的设置，保证了总是出错的“异常点”能够被剔除，从而保证模型的精确性，而在达到最大误分类次数之前，每个误分类点也绝对会被发现，被用于更新w和b的值，不会出现完全不理睬某个样本的不公平的训练。

训练过程改为以下的代码：

```python
def train2(trainSet, learningRate):
    w = [ 0 for i in range(40)]
    b = 0
    flag = False
    failSample = {}
    while flag == False:
        flag, w, b = PLA2(trainSet, learningRate, w, b, failSample)
    return w, b
```

`flag`用于判断是否出现了误分类点，如果出现了误分类点则继续迭代，计算新的w和b值，直到没有误分类点为止。而`failSample`为字典，用于记录误分类点的编号和误分类次数。如：`failSample[10]=5`表示第10个样本以及误分类了5次。

每次迭代运行以下代码：

```python
def PLA2(trainSet, learningRate, w, b, failSample):
    c = 0	# c用来记录样本编号，从1开始计数。
    # 遍历每个样本
    for eachSample in trainSet:
        c += 1
        # 计算当前样本的预测值
        num = 0.0
        for i in range(40):
            num = num + w[i] * eachSample[i]
        num += b
        # 如果预测值和真实值不符则更新w和b并重新检验
        if num==0 or (num>0 and eachSample[40]==-1) or (num<0 and eachSample[40]==1):
            # 更新误分类点的误分类次数，如果误分类次数大于阈值则直接跳过
            if c not in failSample.keys():	# 第一次出现的误分类点
                failSample[c] = 1
            elif failSample[c] < 5:			# 没达到阈值的误分类点
                failSample[c] += 1
            else:							# 达到阈值的误分类点
                continue
			# 更新w和b
            for i in range(40):
                w[i] = w[i] + eachSample[40] * eachSample[i] * learningRate
            b += (learningRate * eachSample[40])
            return False, w, b
    # 所有点全部正确分类后才返回True表示结束
    return True, w, b
```

经测试，这种方法的耗时远大于设置最大迭代次数的耗时。实际上，即便设置较大的最大迭代次数，在我的测试中，在两三百次迭代后w和b的值就会基本收敛。而设置误分类最大次数会极大增加迭代次数。然而，我的改进方法的准确率的确要比原来的算法高。详细的数据分析见下面的实验结果及分析。

-----------

### 5. 实验结果及分析

在7000条样本中，我选取了前6000条作为训练集，后1000条作为验证集。

首先说明实验要求的PLA算法下的结果。有两个参数可以进行调整：学习率和迭代次数。其中，迭代次数决定了模型是否精确地贴合了训练集的分布。在每次w和b更新后输出二者的值，如果二者几乎没有变化则说明参数已经收敛，更多的迭代次数也几乎没用。而迭代次数过少则模型会欠拟合，显然不利于模型的精度。理论上迭代次数越大越好，尽管过大没有必要。所以下面只讨论学习率对结果的影响，保证在不同的学习率下，迭代次数为10000。

针对不同的学习率，我测试了最后的准确率，结果如下：

| 学习率 | 10    | 1     | 0.1   | 0.01  | 0.001 | 0.0001 | 0.00001 |
| ------ | ----- | ----- | ----- | ----- | ----- | ------ | ------- |
| 准确率 | 58.8% | 58.0% | 58.8% | 58.8% | 58.0% | 58.8%  | 58.8%   |

可以看出，在这种情况下，学习率对准确率的影响不大，几乎是没有影响。原因我认为正如之前分析的，在每次遇到误分类点之后更新权值，重新迭代时又从头开始找误分类点，之后的多数样本都训练不到。少量的样本的学习导致了欠拟合，不管学习率如何，模型本身就不能很好地反映样本的分布，准确率自然也就反常。不过考虑到验证集中有51%的1标签，49%的-1标签，准确率也已经高于了随机值，也就是说模型训练的确还是起到了一定效果的。

接着是我的创新算法的分析，有两个参数可以进行调整：学习率和最大误分类次数。

先设置学习率为1，对不同的最大误分类次数，准确率如下：

| 最大误分类次数 | 1     | 2     | 3     | 4     | 5     | 7     | 10    | 20    |
| -------------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| 准确率         | 75.4% | 76.0% | 77.1% | 76.2% | 76.4% | 76.3% | 76.2% | 76.5% |

![](pic\\2.png)

可以看到，在最大误分类次数较小时，出现了明显的准确率峰值，在之后准确率几乎不变。这种结果可以想见：在次数较小时，某些不是比较异常的点被剔除，造成了模型的不准确，而次数较大时，异常点已经多次用于更新模型参数，异常点已经对模型因产生了较大影响，同样也使得模型不精确。

在误分类次数为3时准确率达到最大值。在改次数下，对不同的学习率进行测试：

| 学习率 | 10    | 1     | 0.1   | 0.01  | 0.001 |
| ------ | ----- | ----- | ----- | ----- | ----- |
| 准确率 | 77.1% | 77.1% | 77.1% | 77.1% | 77.1% |

在误分类次数为3的前提下改变学习率，在上述的学习率下准确率居然完全没有改变。我觉得应该是在不同的学习率下，剔除的异常点每次都相同，剩下的样本相同且能够被线性分类。考虑到模型参数必然会收敛，小学习率只是会影响收敛快慢，而不会影响模型参数结果，从而导致了相同的结果。

## 二、 逻辑回归

### 1. 算法原理

和PLA一样，逻辑回归也是通常针对二分类问题，输入是样本的特征向量$\bold x\in R^n$，但输出是样本属于某一个类别的概率而不是确切的类别。

概率预测使用的是logistic函数：
$$
F(x)=\frac{1}{1+e^{-x}}
$$
函数单调递增，取值为$(0,+1)$，当x值为$-\infin$则函数值为0，x值为$+\infin$则函数值为1，0到1之间的取值正好能作为概率。

如果样本的结果为$y\in\{0,1\}$，则逻辑回归中，结果属于1的概率为：
$$
P(y=1|x)=\frac{\exp(w\cdot x+b)}{\exp(w\cdot x+b)+1}
$$
考虑到偏置b=b*1，将权重向量w增加一维，值为b，特征向量x增加一维，值为1，就能写成：
$$
P(y=1|x)=\frac{\exp(w\cdot x)}{\exp(w\cdot x)+1}=\frac{1}{\exp(-w\cdot x)+1}
$$
预测结果为0的概率为：
$$
P(y=0|x)=1-P(y=1|x)=1-\frac{1}{\exp(-w\cdot x)+1}
$$
令y=1的概率函数为$\pi(x)$，则样品x属于某个类别y的概率表示为：
$$
f(x)=P(y|x)=\pi(x)^y(1-\pi(x))^{1-y}
$$
$f(x)$称为似然函数。整个训练集上，似然函数为：
$$
\Pi^N_{i=1}\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}
$$
为了减小不确定性，则需要对似然函数值取得最大值。为了方便计算可以对似然函数取对数：
$$
L(w)=\sum^N_{i=1}[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
=\sum^N_{i=1}[y_i(w\cdot x_i)-\log(1+e^{w\cdot x_i})]
$$
对$L(w)$取负值作为逻辑回归的损失函数，并使用梯度下降法进行优化。对$-L(w)$对w求导得：
$$
-\nabla_wL(w)=-\sum^N_{i=1}[y_i-\pi(x_i)]x_i
$$
更新参数则使用以下公式：
$$
w=w+\eta*\frac{1}{N}\sum^N_{i=1}[y_i-\pi(x_i)]x_i
$$
上述公式中，$\frac{1}{N}$的部分通常和学习率$\eta$合并，不会写出来。


-------

### 2. 流程图和伪代码

![](pic\\3.png)

和之前类似，每个训练样本存储在一个列表中，前40个值为属性值，第41个值为1，第42个值为标签。

```pseudocode
/*
input:训练集trainSet, 学习率learningRate
output:训练好的w
*/
def train(trainSet, learningRate){
    初始化w
    c=0
    while (c != 最大迭代次数)
        c += 1
        L = 0 /* 损失函数梯度 */
        for (eachSample in trainSet)
            计算整个训练集上的L
		end
        w = w + learningRate * L
    end
    return w
}
```

------

### 3. 代码展示

读取训练集的代码如下：

```python
# 读取训练集
def readTrainSet():
    trainSet = []
    with open('train.csv','r') as ts:
        for eachLine in ts:
            s = eachLine.split(',')
            # 去掉最后的回车
            s[40] = s[40][0:-1]
            # 进行数据类型转换，从字符串转换成数字
            for i in range(40):
                s[i] = float(s[i])
            s[40] = int(s[40])
            # 添加一维常数项,在下标为40的位置，41的位置为标签
            s.append(s[40])
            s[40] = 1
            # 加入记录训练集的列表
            trainSet.append(s)
    return trainSet
```

其中，读取的时候需要将每个样本存储到列表`s`中。`s.append(s[40])`使得第42维为标签，`s[40] = 1`使得第41维为1。同样，使用数据类型转换将特征值改为`float`，将标签改为`int`。

训练过程如下：

```python
def train(trainSet, learningRate):
    w = [ 0 for i in range(41)]
    c=0
    #设置最大迭代次数
    while c != 200:
        c += 1
        
        # 计算损失函数梯度
        L = [0 for i in range(41)] 				# L为损失函数的梯度，41维
        for eachSample in trainSet:
            pix = 0                             # 用于计算pi(x)
            for i in range(41):
                pix += ( - w[i] * eachSample[i] )
            pix = 1 / (1 + math.exp(pix) )
            for i in range(41):
                L[i] += (eachSample[41] - pix) * eachSample[i]
		# 更新权值w。之后的除7000为样本数，相当于算在学习率中
        for i in range(41):
            w[i] = w[i] + learningRate * L[i] / 7000
    return w
```

-----

### 4. 实验结果及分析

同样，本实验也有学习率和最大迭代次数两个参数。考虑到迭代次数不足会欠拟合，对模型的训练没有意义，迭代次数过多只会增多训练时间，收敛后不会影响模型精度，所以只对学习率进行调参。迭代次数为300次。

注意：以下的学习率在实际程序中都还除以了7000，即训练样本的大小。在公式中该值被并入学习率，但在实际运算时需要考虑。

| 学习率*7000 | 10    | 1     | 0.1   | 0.01  | 0.001 |
| ----------- | ----- | ----- | ----- | ----- | ----- |
| 准确率      | 50.2% | 58.6% | 76.1% | 76.2% | 72.7% |

![](pic\\4.png)

上面图片的横坐标x表示的数值为$10^x$。

可以看到，在较小学习率下，准确率不是最高的，因为参数数值没有收敛。而较大的学习率下，难以得到损失函数的极小值点，准确率很低。在中等的学习率下才能保证较小的迭代次数和较优的准确率。

---------

## 四、 思考题

1. 不同的学习率对模型收敛有何影响？从收敛速度和是否收敛两方面来回答。

   在较小的学习率下，每次参数更新的值更小，从而减小了收敛速度。但是如果迭代次数足够多，就能够在允许范围内收敛。而如果学习率较大，参数更新值更大，收敛速度也就更快。但是参数值更新较大时，在损失函数的极小值点附近，参数决定的点会左右横跳，难以取得极小值点，从而难以收敛。

2. 使用梯度的模长是否为零作为梯度下降的收敛终止条件是否合适，为什么？一般如何判断模型收敛？

   梯度的模长是否为零作为梯度下降的收敛终止条件并不合适。模型不可能完美拟合训练数据，梯度不可能完全为0。这样的收敛条件难以满足。收敛条件可以设置为梯度的模长小于某个允许值，当梯度模长较小时，参数更新变化不大，基本可以判断收敛。